{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":8263220,"sourceType":"datasetVersion","datasetId":4904844}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport numpy as np\nimport pandas as pd\nimport json\nimport pydoc\nimport sys\nimport torch\nimport tqdm\nimport transformers\nimport urllib\n\nfrom pydoc import locate\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom urllib.parse import urlparse\n\ntest_data_file = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\nweghts_path = \"/kaggle/input/piid-modelweights/\"\nchunk_size = 5_000\nscaler = 0.02\ntokenizer_stride = 32\n\ndevice = [\"cpu\", \"cuda\"][1]\n\nmodel_params = [\n    {\"path\":\"model_387\", \"max_tokens\":512},\n    {\"path\":\"model_539\", \"max_tokens\":1024},\n    {\"path\":\"model_543\", \"max_tokens\":1024},\n    {\"path\":\"model_560\", \"max_tokens\":2048},\n    {\"path\":\"model_563\", \"max_tokens\":2048},\n    {\"path\":\"model_572\", \"max_tokens\":1024},\n]","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:48:05.919213Z","iopub.execute_input":"2024-05-24T09:48:05.919836Z","iopub.status.idle":"2024-05-24T09:48:13.335208Z","shell.execute_reply.started":"2024-05-24T09:48:05.919804Z","shell.execute_reply":"2024-05-24T09:48:13.334235Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Decode variables","metadata":{}},{"cell_type":"code","source":"def chunk(l, n): \n    for i in range(0, len(l), n):  \n        yield l[i:i + n] \n\ndef decode_targets(targets:list, doc_ids:list) -> list:\n    df = pd.DataFrame({\n        \"target\": targets,\n        \"document\": doc_ids\n    })\n    df[\"prev_target\"] = df.groupby(\"document\")[\"target\"].shift(1).values\n    cond = (df[\"prev_target\"] == df[\"target\"]) & (~df[\"prev_target\"].isnull())\n    df[\"target\"] += 100*cond.astype(int)\n\n    df[\"target\"] = df[\"target\"].map({\n        0: \"O\",\n        1: \"B-NAME_STUDENT\",\n        2: \"B-URL_PERSONAL\",\n        3: \"B-ID_NUM\",\n        4: \"B-EMAIL\",\n        5: \"B-STREET_ADDRESS\",\n        6: \"B-PHONE_NUM\",\n        7: \"B-USERNAME\",\n        100: \"O\",\n        101: \"I-NAME_STUDENT\",\n        102: \"I-URL_PERSONAL\",\n        103: \"I-ID_NUM\",\n        104: \"I-EMAIL\",\n        105: \"I-STREET_ADDRESS\",\n        106: \"I-PHONE_NUM\",\n        107: \"I-USERNAME\",\n    })\n\n    return df[\"target\"].values.tolist()\n\nwith open(test_data_file) as f:\n    test = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:48:18.835755Z","iopub.execute_input":"2024-05-24T09:48:18.836929Z","iopub.status.idle":"2024-05-24T09:48:18.858295Z","shell.execute_reply.started":"2024-05-24T09:48:18.836875Z","shell.execute_reply":"2024-05-24T09:48:18.857330Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_batch_record(record, tokenizer, max_n_tokens, stride):\n    max_len = min(tokenizer.model_max_length, max_n_tokens)\n    tokenized_inputs = tokenizer(record[\"tokens\"], return_offsets_mapping=False,\n                                verbose=False, is_split_into_words=True, add_special_tokens=True,\n                                max_length=max_len, stride=stride, truncation=True, return_overflowing_tokens=True)\n\n    if \"labels\" in record:\n        targets_map = {i:v for i,v in enumerate(encode_targets(record[\"labels\"]))}\n    else:\n        targets_map = {}\n\n    batch = [{\n        \"input_ids\": tokenized_inputs[\"input_ids\"][i],\n        \"attention_mask\": tokenized_inputs[\"attention_mask\"][i],\n        \"word_ids\": [-100 if x is None else x for x in tokenized_inputs.word_ids(i)],\n        \"targets\": [targets_map.get(x, -100) for x in tokenized_inputs.word_ids(i)],\n        \"document\": [record[\"document\"]] * len(tokenized_inputs[\"input_ids\"][i]),\n    } for i in range(len(tokenized_inputs[\"input_ids\"]))]\n\n    return batch\n\ndef tokenize_and_batch(sample, tokenizer, max_n_tokens, stride):\n    tokenized_sample = [tokenize_and_batch_record(rec, tokenizer, max_n_tokens, stride) for rec in tqdm(sample)]\n    tokenized_sample = [x for xs in tokenized_sample for x in xs]\n    return tokenized_sample\n\nclass ListDataset(Dataset):\n    def __init__(self, data_list):\n        self.data = data_list\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\ndef collate_fn(batch):\n    keys = batch[0].keys()\n    seq = {k:[torch.tensor(x[k]) for x in batch] for k in keys}\n    return {k:torch.nn.utils.rnn.pad_sequence(v, True, 0) for k,v in seq.items()}\n    \ndef create_dataloader(test_data, max_n_tokens, tokenizer_stride):\n    tokenizer = AutoTokenizer.from_pretrained(weghts_path + \"tokenizer\")\n    tokenized_data = tokenize_and_batch(test_data, tokenizer, max_n_tokens, tokenizer_stride)\n    return DataLoader(\n        ListDataset(tokenized_data),\n        batch_size = 4,\n        num_workers = 2,\n        pin_memory = True,\n        shuffle = False,\n        collate_fn = collate_fn,\n    )\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:48:22.100234Z","iopub.execute_input":"2024-05-24T09:48:22.101120Z","iopub.status.idle":"2024-05-24T09:48:22.117681Z","shell.execute_reply.started":"2024-05-24T09:48:22.101090Z","shell.execute_reply":"2024-05-24T09:48:22.116569Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Prediction phase","metadata":{}},{"cell_type":"code","source":"class HFModel(torch.nn.Module):\n    def __init__(self, load_path):\n        super().__init__()\n        self.backbone = AutoModelForTokenClassification.from_pretrained(load_path)\n\n    def forward(self, d):\n        preds = self.backbone(input_ids = d[\"input_ids\"], attention_mask = d[\"attention_mask\"])\n        return {\"logits\": preds.logits}\n\ndef results_to_df(predictions, data):\n    probs = torch.nn.functional.softmax(predictions[\"logits\"], -1)\n    probs = probs.flatten(0,1).cpu().numpy()\n    probs = pd.DataFrame(probs, columns = [f\"prob_{i}\" for i in range(8)])\n    res = pd.DataFrame({\n        \"document\": data[\"document\"].cpu().flatten(),\n        \"word_ids\": data[\"word_ids\"].cpu().flatten(),\n        \"targets\": data[\"targets\"].cpu().flatten(),\n    })\n    res = pd.concat([res, probs], axis=1)\n    return res\n\ndef inference(model_cfg, test_data):\n    model = HFModel(f\"{weghts_path}/{model_cfg['path']}\")\n    model.to(device)\n    model.eval()\n    \n    dl = create_dataloader(test_data, model_cfg['max_tokens'], tokenizer_stride)\n\n    res_df = pd.DataFrame()\n    with torch.no_grad():\n        for data in dl:\n            for k in data.keys():\n                data[k] = data[k].to(device)\n            preds = model(data)\n            res_df = pd.concat([res_df, results_to_df(preds, data)])\n    return res_df\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:48:28.045918Z","iopub.execute_input":"2024-05-24T09:48:28.046294Z","iopub.status.idle":"2024-05-24T09:48:28.059205Z","shell.execute_reply.started":"2024-05-24T09:48:28.046264Z","shell.execute_reply":"2024-05-24T09:48:28.057924Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Generate submission","metadata":{}},{"cell_type":"code","source":"test_df = None\n\nfor test_chunk in chunk(test, chunk_size):\n    preds_df = None\n    for cfg in model_params:\n        preds_df = pd.concat([preds_df, inference(cfg, test_chunk)])\n\n    preds_df = preds_df[preds_df[\"word_ids\"] != -100]\n    preds_df = preds_df.groupby([\"document\", \"word_ids\"]).agg(**{f\"prob_{i}\":(f'prob_{i}', 'mean') for i in range(8)}).reset_index()\n\n    test_chunk_df = {}\n    test_chunk_df[\"tokens\"] = [x for xs in [rec[\"tokens\"] for rec in test_chunk] for x in xs]\n    test_chunk_df[\"document\"] = [x for xs in [[rec[\"document\"]]*len(rec[\"tokens\"]) for rec in test_chunk] for x in xs]\n    test_chunk_df[\"word_ids\"] = [x for xs in [range(len(rec[\"tokens\"])) for rec in test_chunk] for x in xs]\n    test_chunk_df = pd.DataFrame(test_chunk_df)\n\n    probs = preds_df[[f\"prob_{i}\" for i in range(8)]].values\n    probs[:,0] *= scaler\n    preds_df[\"preds\"] = probs.argmax(-1)\n\n    test_chunk_df = test_chunk_df.merge(preds_df[[\"document\", \"word_ids\", \"preds\"]], how = \"left\")\n    test_chunk_df[\"preds\"] = test_chunk_df[\"preds\"].fillna(0).astype(int)\n    \n    # Formatting, 1st step\n    test_chunk_df.loc[test_chunk_df[\"tokens\"] == \"\\n\", \"preds\"] = 5\n\n    # Formatting, 2nd step\n    test_chunk_df.loc[(test_chunk_df[\"preds\"] == 1) & (~test_chunk_df[\"tokens\"].str.istitle()), \"preds\"] = 0\n    test_chunk_df = test_chunk_df.merge(test_chunk_df.groupby([\"document\", \"tokens\"])[\"preds\"].max().rename(\"max_preds\").reset_index(), how=\"left\")\n    test_chunk_df.loc[test_chunk_df[\"max_preds\"] == 1, \"preds\"] = 1\n\n    test_chunk_df[\"label\"] = decode_targets(test_chunk_df[\"preds\"].values, test_chunk_df[\"document\"].values)\n    test_chunk_df = test_chunk_df[test_chunk_df[\"label\"] != \"O\"][[\"document\", \"word_ids\", \"label\"]].rename({\"word_ids\":\"token\"}, axis=1)\n\n    test_df = pd.concat([test_df, test_chunk_df])\n\n\ntest_df = test_df.reset_index(drop=True)\nsub_df = test_df.reset_index().rename({\"index\":\"row_id\"}, axis=1)\n\nsub_df.to_csv(\"submission.csv\", index=False)\nsub_df","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:51:57.204027Z","iopub.execute_input":"2024-05-24T09:51:57.204496Z","iopub.status.idle":"2024-05-24T09:52:23.863608Z","shell.execute_reply.started":"2024-05-24T09:51:57.204459Z","shell.execute_reply":"2024-05-24T09:52:23.862263Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a77c7610abd84f81a28c3afa63c7a72c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd86a9d09ef48f8a51412697f8cb1d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35867d6212a045458104ca92bb2f3dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5d6605f7f864401a5779eaef98323af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"406a0bf52834416a8efda6ff76116266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ac7ddf971a43e3842d95d81940cd80"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    row_id  document  token           label\n0        0         7      9  B-NAME_STUDENT\n1        1         7     10  I-NAME_STUDENT\n2        2         7    482  B-NAME_STUDENT\n3        3         7    483  I-NAME_STUDENT\n4        4         7    741  B-NAME_STUDENT\n5        5         7    742  I-NAME_STUDENT\n6        6        10      0  B-NAME_STUDENT\n7        7        10      1  I-NAME_STUDENT\n8        8        10    464  B-NAME_STUDENT\n9        9        10    465  I-NAME_STUDENT\n10      10        16      4  B-NAME_STUDENT\n11      11        16      5  I-NAME_STUDENT\n12      12        20      5  B-NAME_STUDENT\n13      13        20      6  I-NAME_STUDENT\n14      14        56     12  B-NAME_STUDENT\n15      15        56     13  I-NAME_STUDENT\n16      16        86      6  B-NAME_STUDENT\n17      17        86      7  I-NAME_STUDENT\n18      18        93      0  B-NAME_STUDENT\n19      19        93      1  I-NAME_STUDENT\n20      20       104      7  B-NAME_STUDENT\n21      21       104      8  I-NAME_STUDENT\n22      22       104      9  I-NAME_STUDENT\n23      23       112      5  B-NAME_STUDENT\n24      24       112      6  I-NAME_STUDENT\n25      25       123     32  B-NAME_STUDENT\n26      26       123     33  I-NAME_STUDENT","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>7</td>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>7</td>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>7</td>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>7</td>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>7</td>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>7</td>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>10</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>10</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>10</td>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>10</td>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>16</td>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>16</td>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>20</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>20</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>56</td>\n      <td>12</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>56</td>\n      <td>13</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>86</td>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>86</td>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>93</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>93</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>104</td>\n      <td>7</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>104</td>\n      <td>8</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>104</td>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>112</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>112</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>123</td>\n      <td>32</td>\n      <td>B-NAME_STUDENT</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>123</td>\n      <td>33</td>\n      <td>I-NAME_STUDENT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Successfully saved as CSV file\")","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:55:24.824107Z","iopub.execute_input":"2024-05-24T09:55:24.824518Z","iopub.status.idle":"2024-05-24T09:55:24.830486Z","shell.execute_reply.started":"2024-05-24T09:55:24.824483Z","shell.execute_reply":"2024-05-24T09:55:24.829315Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Successfully saved as CSV file\n","output_type":"stream"}]}]}